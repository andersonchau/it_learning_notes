\documentclass{article}
\usepackage{amsmath}
\usepackage{blindtext}
\begin{document}

\section{Foreword}
Author : Anderson Chau
\newline 
\newline 
Disclaimer : This notes is written only for my memorization purpose after I have studied online lecture notes and blogs. 
% ######################################################################
% ######################################################################
% ######################################################################
\section{Confusion Matrix}
Predict True, Actual True : True Positive (TP) \newline
Predict True, Actual False : False Positive (FP) \newline
Predict False, Actual False : False Negative (FN)\newline 
Predict False, Actual True : True Negative (TN)\newline
\newline
\newline
Accuracy = (TP+FN)/(TP+FP+FN+TN), performance of correct classification
\newline
\newline
Precision = TP / (TP+FP)  ( correctly classified as positive / Everything classified as positive ), used when Cancer detection. (We don't want to initiate cancer treatment if the person is actually healthy).  
\newline
\newline
Recall = TP / (TP + FN) ( correctly classified as positive / Actually positive ), FP is more expensive than TN . (e.g. Fraud detection).
\newline
\newline
Note : Mathematically, Precision and Recall are inverse relationship, there is a trade off between recall and precision.
\newline
\newline
F1 score = 2(P*R)/(P+R), a compromised metric
 
\section{K-Nearest Neighbour}
Description : Choose the *majority* class of nearest (e.g. Eclidean Distance ) K data an classify it. 
\newline
How to Choose K(hyper-paramaeter) : General rule of thumb : sqrt(number of data)/2 or by searching and comparing different k's for highest prediction accuracy.
\newline 
Normalization of data in preprocessing is a must


\section{K-Means Clustering}
Simple Description : Identify clusters by finding the centroid of data points
\newline
\newline
Algorithm :  
\newline
\newline
1. Initialize \( \mu_1 \),\( \mu_2 \).... \( \mu_k \) randomly (k is hyper-parameter) 
\newline
\newline
2. Repeated until converge : 
\newline
\newline
(i) \( $$c^{(i)} := \arg \min_j ||x^{(i)} - \mu_j||^2$$ , j \in [1:k]\) , (i.e. \(c^{(i)}\) denote which  \( \mu \)  the \(x^{(i)}\) is linked to. Link each data point to nearest  \( \mu_j \). If \( x^{(i)}\) is nearest to \( \mu_s \), then \(c^{(i)} = j\). Thus, k partitions are created.  )
\newline
\newline
(ii) \($$\mu_j := \frac{\sum_{i=1}^m 1\{{c^{(i)} = j}\} x^{(i)}}{\sum_{i=1}^m 1\{{c^{(i)} = j}\}}$$\) , (i.e. For each data point in each partition from (i) , find the new centroid and assign to \(\mu_k\)
\newline
\newline 
Proof of convergence of the algorithm : consider 
\newline 
\newline 
\( $$J(c, \mu) = \sum_{i=1}^m ||x^{(i)} - \mu_{c^{(i)}}||^2$$\)
\newline 
Observation : J must be monotonically decreasing. It is because for step (i) It is adjusting \(c^(i)\) to reduce J, for step (ii) we are adjusting  \(\mu_j\) to reduce J
\newline 
J is non-convext, it may get to local minimum. To try several random initial values, and choose the lowest J.
% ######################################################################
% ######################################################################
% ######################################################################
\section{Linear Regression(MSE approach) }
Hypothesis : 
\[
h_\theta(x) = \sum_j \theta_j x_j = \theta^\top x
\]
We want to minimize MSE (Mean Square Error) 
\[
J(\theta) = \frac{1}{2} \sum_i \left( h_\theta(x^{(i)}) - y^{(i)} \right)^2 = \frac{1}{2} \sum_i \left( \theta^\top x^{(i)} - y^{(i)} \right)^2
\]
Gradient of J : 
\[
\frac{\partial J(\theta)}{\partial \theta_j} = \sum_i x_j^{(i)} \left( h_\theta(x^{(i)}) - y^{(i)} \right)
\]
Each \(\theta_j\) is updated for each step by gradient descent algorithm. 
\[
\theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta)
\]
Practically : 
\newline
(i) Learning rate \(\alpha\) is hyperparameter and data dependent , larger, fewer steps to get to min. but may miss the minimum. (Monitor the loss curve, J value vs iteration ). 
\newline
(ii) Batch GD is slow, may be Mini-Batch GD or Stochastic GD.
\newline
(iii) If \(\alpha\) is small but the loss oscillate , converged and stop learning.


\section{Linear Regression(MLE approach) }
\section{Logistic Regression}
\[P(y=1|x) = h_\theta(x) = \frac{1}{1 + \exp(-\theta^\top x)} \equiv \sigma(\theta^\top x)\]
\[P(y=0|x) = 1 - P(y=1|x) = 1 - h_\theta(x)\]
Loss function is 
\[J(\theta) = -\sum_i \left[ y^{(i)} \log(h_\theta(x^{(i)})) + (1 - y^{(i)}) \log(1 - h_\theta(x^{(i)})) \right]\]
Again , BGD for following gradient of J : 
\[\frac{\partial J(\theta)}{\partial \theta_j} = \sum_i x^{(i)}j \left( h\theta(x^{(i)}) - y^{(i)} \right)\]
Interpretation : For a particular sample : if h return 1/0 and y return 1/0 , the term is 0. if  h return 1/0 and y return 0/1 , the term is positive infinity. 
\section{Logistic Regression(MLE approach)}
\section{Softmax Regression(Multi-Class Logistic)}
k classes, n x k parameters , and the hypothesis is :

\begin{align}
h_\theta(x) =
\begin{bmatrix}
P(y = 1 | x; \theta) \\
P(y = 2 | x; \theta) \\
\vdots \\
P(y = K | x; \theta)
\end{bmatrix}
=
\frac{1}{ \sum_{j=1}^{K}{\exp(\theta^{(j)\top} x) }}
\begin{bmatrix}
\exp(\theta^{(1)\top} x ) \\
\exp(\theta^{(2)\top} x ) \\
\vdots \\
\exp(\theta^{(K)\top} x ) \\
\end{bmatrix}
\end{align}

Below Loss function  is simple to understand : By referencing to previous hpothesis, we want to maximize the y=k associated probility. 
\[J(\theta) = -\left[ \sum_{i=1}^m \sum_{k=1}^K 1\{y^{(i)}=k\} \log \left( \frac{\exp(\theta^{(k)\top} x^{(i)})}{\sum_{j=1}^K \exp(\theta^{(j)\top} x^{(i)})} \right) \right]\]
Gradient of J is, we solve the problem by  GD : 
\[\nabla_{\theta^{(k)}} J(\theta) = -\sum_{i=1}^m \left[ x^{(i)} \left( 1\{y^{(i)}=k\} - P(y^{(i)}=k|x^{(i)};\theta) \right) \right]\]
Where : 
\[P(y^{(i)}=k|x^{(i)};\theta) = \frac{\exp(\theta^{(k)\top} x^{(i)})}{\sum_{j=1}^K \exp(\theta^{(j)\top} x^{(i)})}\]


% ######################################################################
\section{BGD variation : Mini BGD/SGD}
BGD use all training data in a single step, which is exteremely costly.
% ######################################################################
\section{Loss function in Classification(Binary) Problem - General treatment}
General Hypothesis : \(h_\theta(x) = x^T \theta\)
\newline
\newline
Adjustment for binary classification : 
\newline
\(
\text{sign}(h_\theta(x)) = \text{sign}(\theta^T x)= \text{sign}(t) = 
\begin{cases} 
1 & \text{if } t > 0 \\
0 & \text{if } t = 0 \\
-1 & \text{if } t < 0 
\end{cases}
\)
\newline
\newline
Measure of confidence :  \(h_\theta(x) = x^T \theta\) gives larger value, more confident  
\newline
\newline
Margin ( \(y x^T \theta\) ) : (i) if \(h_\theta(x)\) classify correctly, margin is positive, otherwise negative.
\newline
(ii) Therefore our objective is to maximize the margin ( we want both correct classification and be confident)
\newline
\newline
Consider the following loss function : 
\[
J(\theta) = \frac{1}{m} \sum_{i=1}^{m} \phi\left(y^{(i)} \theta^T x^{(i)}\right)
\]

We want penalize wrong classfication and encourage correct one , we design \(\phi\) as 
\(
\phi(z) \to 0 \text{ as } z \to \infty, \quad \text{while} \quad \phi(z) \to \infty \text{ as } z \to -\infty
\)  where  \(z = yx^{T}\theta\) , and examples are :  
\newline 
\newline
logistic loss : \(\phi_{\text{logistic}}(z) = \log(1 + e^{-z})\) ,used in logistic regression 
\newline 
\newline 
hinge loss : \(\phi_{\text{hinge}}(z) = [1 - z]_+ = \max{1 - z, 0}\), used in SVM
\newline 
\newline 
Exponential loss \(\phi_{\text{exp}}(z) = e^{-z}\), used in boosting 
\section{Kernel Mapping (Special case demo by Linear Regression + Polymoninal Kernel)}
(I) Purpose : To x map from lower higher dimension. Useful when data are non-linearly separable(Transform to a curve) 
\newline
(II) Computation complexity does not necessarily increase proportionately.
\newline
(III) Example : a mapping function \(\varphi : R \to R^{4}\) , \(x \to [1,x,x^{2},x^{3}]\), and h is \(\theta^{T}x\) having  \(\theta  = [\theta_1,\theta_2,\theta_3,\theta_4]\) 
\newline
(IV) Terms : x is called attribute, \(x \to [1,x,x^{2},x^{3}]\) called feature, \(\varphi\) feature map, \(\varphi : R^{1} \to R^{4}\) in this case. d =1 p = 4 
\newline
(IV) Another Example : a mapping function \(\varphi : R^{3} \to R^{1000}\) , \(x \to [1,x_1,x_1^{2},x_1^{3},x_1 x_2,x_1 x_2^{2} .... ]\) (*) ,let d  = 3 , p =1000. If we exhaust all possibilities, then p = 1 + d + \(d^2\) + \(d^3\)  (**) 
\newline
Recall GD stepping  : 
\[\theta := \theta + \alpha \sum_{i=1}^{n} (y^{(i)} - h_{\theta}(x^{(i)}))x^{(i)}\]
\[\theta := \theta + \alpha \sum_{i=1}^{n} (y^{(i)} - \theta^Tx^{(i)})x^{(i)}\]
Putting kernel mapping to the equation :
\[\theta := \theta + \alpha \sum_{i=1}^{n} (y^{(i)} - \theta^T\phi(x^{(i)}))\phi(x^{(i)})\]

We pause here to evaluate the cost of computing each of update (Curse of Demensionality...), considering (**). If we just use the kernel direction, we suffer the curse of demensionality : Suppose d (data dimension) = 1000, then by using the mapping in (**) we have p = \(10^9\). \(\theta^T\phi(x^{(i)})\) need O(p) (dot product ) , and O(np) for summing up all data in each step.
\newline
Going back to BGD.
\[\theta := \theta + \alpha \sum_{i=1}^{n} (y^{(i)} - \theta^T\phi(x^{(i)}))\phi(x^{(i)})\]
, assuming 
\(\theta = \sum_{i=1}^{n} \beta_i \phi(x^{(i)})\) (*) at some point, with initialization \(\theta = 0 = \beta \)
\newline 
It becomes 
\[\theta := \sum_{i=1}^{n} \beta_i \phi(x^{(i)}) + \alpha \sum_{i=1}^{n} (y^{(i)} - \theta^T\phi(x^{(i)}))\phi(x^{(i)})\] 
Rearranging : 
\[\theta := \sum_{i=1}^{n} (\beta_i + \alpha  (y^{(i)} - \theta^T\phi(x^{(i)})))\phi(x^{(i)})\] 
Therefore it is equivalent to updating \(\beta_i\) ( instead of \(\theta_i\) ) by 
\[\beta_i := \beta_i + \alpha  (y^{(i)} - \theta^T\phi(x^{(i)}))\]
by (*) above
\[ \beta_i := \beta_i + \alpha \left( y^{(i)} - \sum_{j=1}^{n} \beta_j \phi(x^{(j)})^T \phi(x^{(i)}) \right)\]
Computing of LHS is fast because : (1) we can pre-compute \( \phi(x^{(j)})^T \phi(x^{(i)})\) for all i,j, and (2) \( \phi(x^{(j)})^T \phi(x^{(i)})\) can be represented by \(<x^{(i)},x^{(j)}>\)  : 
\newline
\(\langle \phi(x), \phi(z) \rangle = 1 + \sum_{i=1}^{d} x_i z_i + \sum_{i,j \in \{1, \ldots, d\}} x_i x_j z_i z_j + \sum_{i,j,k \in \{1, \ldots, d\}} x_i x_j x_k z_i z_j z_k = 1 + \langle x, z \rangle + \langle x, z \rangle^2 + \langle x, z \rangle^3\) (**)
\newline 
\newline 
Define K where K is n x n ( n is the number of training samples) matrix, with \(K(x, z) = \langle \phi(x), \phi(z) \rangle\) , where \(K_ij\) is \(\langle \phi(x^{(i)}), \phi(x^{(j)}) \rangle\)
\newline\newline
Therefore, the process is : (1) compute \(K_ij\) using (**) , \(\text{for all } i, j \in \{1, \ldots, n\}. \quad \text{Set } \beta := 0\) ,
\newline
 (2) Loop
\[\forall i \in \{1, \ldots, n\}, \quad \beta_i := \beta_i + \alpha \left( y^{(i)} - \sum_{j=1}^{n} \beta_j K(x^{(i)}, x^{(j)}) \right)\] 
in vectorized notation: 
\[\beta := \beta + \alpha (\tilde{y} - K\beta)\]
When doing inference : 
\[\theta^T \phi(x) = \sum_{i=1}^{n} \beta_i \phi(x^{(i)})^T \phi(x) = \sum_{i=1}^{n} \beta_i K(x^{(i)}, x)\]
In practice, we do computation using K ( at O(d) cost ) instead of directly from \(\phi(x)\) is much faster. Further, We only need to know K but "just only need to know" the existence of \(\phi(x)\). There is no need to be able to write down \(\phi(x)\). Consider the Kernel applied to bitmap : number of bits as d. (Great reduction!)
\newline  
Intuitively, K represents similarity matrix, i.e. K is small if \( \phi(x^{(j)})^T \phi(x^{(i)})\) is small
\newline
Example : Gaussian Kernel, it can support inifinitely dimensional space of mapping.
\[K(x, z) = \exp\left(-\frac{||x - z||^2}{2\sigma^2}\right)\]
\newline 
Mercer Theorem : For K to be a valid Kernel iff K is PSD.
\newline 
Application : To SVM, perceptron, linear regression, and other learning algorithms represented only in inner product \(\langle x,z\rangle\) , then Apply K(x,z)  
\section{Entropy} 
\section{Decision Tree, with Random Forest, Boosting}
\section{Boostrapping}
\section{PCA, Principal Component Analysis}
\section{SVD, Singular Value Decomposition}
\section{SVM, Support Vector Machine}
\section{Backpropagation}
\section{EM Algorithm}
\section{Generative Learning Algorithm}
\section{Reinforcement learning}
\section{MAP (Maximum a Posterior) vs MLE (Maximum Likelihood Estimation)} 
\section{IDP, Indepent Component Analysis}
\section{Bias Variance Analysis} 
\section{Hidden Markov Model}
\section{Apriori}
\section{Recommender System}
\section{Anomaly Detection} 
\section{Perceptron}

\end{document}
