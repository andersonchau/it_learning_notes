\documentclass{article}
\usepackage{amsmath}
\usepackage{blindtext}

\begin{document}

\section{Foreword}
Author : Anderson Chau
\newline 
\newline 
Disclaimer : The notes are written only for my understanding and memorization purpose after I have self-studied those online lecture notes. 


% ######################################################################
% ######################################################################
% ######################################################################
\section{Likelihood and Maximum Likelihood Estimation(MLE)}
We have m sampling data, we attempt to establish a hypothesis \(h(\theta)\) with parameter \(\theta\) to model the data \newline
The term Likelihood denotes the probabilty that particular value \(\theta\) represents the sampling data.\newline 
We want to find the value of parameter(s) which best represent the data(*), the process is called MLE. \newline 

Let f be the pdf of random variable X, \(X_i\) is the value if sample i ,  then \(f(X_i \mid \theta)\) is read as the chance of \(X_i\) happening if value of parmeter is \(\theta)\) \newline  \newline 
Likelihood function : \(L(\theta) = \prod_{i=1}^{m} f(X_i \mid \theta)\) ( assumption here : sampling are independent process). We want to find \(\theta\) that maximize  L , i.e. (*) \newline
Generally, we take log (monotonically increasing function) likelihood to convert multiplications to additions for easier handling  and then find maxima (by partial derivatives = 0) 

\section{Confusion Matrix}
Predict True, Actual True : True Positive (TP) \newline
Predict True, Actual False : False Positive (FP) \newline
Predict False, Actual False : False Negative (FN)\newline 
Predict False, Actual True : True Negative (TN)\newline
\newline
\newline
Accuracy = (TP+FN)/(TP+FP+FN+TN), performance of correct classification
\newline
\newline
Precision = TP / (TP+FP)  ( correctly classified as positive / Everything classified as positive ), example usage : Cancer detection. (We don't want to initiate cancer treatment if the person is actually healthy).  
\newline
\newline
Recall = TP / (TP + FN) ( correctly classified as positive / Actually positive ), FP is more expensive than TN . (e.g. Fraud detection).
\newline
\newline
Note : Mathematically, Precision and Recall are in inverse relationship, there is a tradeoff between recall and precision.
\newline
\newline
F1 score = 2(P*R)/(P+R), a compromised metric

\section{K-Nearest Neighbour}
Description : Choose the *majority* class of nearest (e.g. Eclidean Distance ) K data points and classify it.  
\newline
How to Choose K(hyper-paramaeter) : General rule of thumb : sqrt(number of data)/2 or by searching and comparing different k's for highest prediction accuracy.
\newline 
Normalization of data in preprocessing is a must


\section{K-Means Clustering}
Simple Description : Identify clusters by finding the centroid of data points
\newline
\newline
Algorithm :  
\newline
\newline
1. Initialize \( \mu_1 \),\( \mu_2 \).... \( \mu_k \) randomly (k is hyper-parameter) 
\newline
\newline
2. Repeated until converge : 
\newline
\newline
(i) \( $$c^{(i)} := \arg \min_j ||x^{(i)} - \mu_j||^2$$ , j \in [1:k]\) , (i.e. \(c^{(i)}\) denote which  \( \mu \)  the \(x^{(i)}\) is linked to. Link each data point to nearest  \( \mu_j \). If \( x^{(i)}\) is nearest to \( \mu_s \), then \(c^{(i)} = j\). Thus, k partitions are created.  )
\newline
\newline
(ii) \($$\mu_j := \frac{\sum_{i=1}^m 1\{{c^{(i)} = j}\} x^{(i)}}{\sum_{i=1}^m 1\{{c^{(i)} = j}\}}$$\) , (i.e. For each data point in each partition from (i) , find the new centroid and assign to \(\mu_k\)
\newline
\newline 
Proof of convergence of the algorithm : consider 
\newline 
\newline 
\( $$J(c, \mu) = \sum_{i=1}^m ||x^{(i)} - \mu_{c^{(i)}}||^2$$\)
\newline 
Observation : J must be monotonically decreasing. It is because for step (i) It is adjusting \(c^(i)\) to reduce J, for step (ii) we are adjusting  \(\mu_j\) to reduce J
\newline 
J is non-convext, it may get to local minimum. To try several random initial values, and choose the lowest J.
% ######################################################################
% ######################################################################
% ######################################################################
\section{Linear Regression(MSE approach) }
Hypothesis : 
\[
h_\theta(x) = \sum_j \theta_j x_j = \theta^\top x
\]
We want to minimize MSE (Mean Square Error) 
\[
J(\theta) = \frac{1}{2} \sum_i \left( h_\theta(x^{(i)}) - y^{(i)} \right)^2 = \frac{1}{2} \sum_i \left( \theta^\top x^{(i)} - y^{(i)} \right)^2
\]
Gradient of J : 
\[
\frac{\partial J(\theta)}{\partial \theta_j} = \sum_i x_j^{(i)} \left( h_\theta(x^{(i)}) - y^{(i)} \right)
\]
Each \(\theta_j\) is updated for each step by gradient descent algorithm. 
\[
\theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta)
\]
Practically : 
\newline
(i) Learning rate \(\alpha\) is hyperparameter and data dependent , larger, fewer steps to get to min. but may miss the minimum. (Monitor the loss curve, J value vs iteration ). 
\newline
(ii) Batch GD is slow, may be Mini-Batch GD or Stochastic GD.
\newline
(iii) If \(\alpha\) is small but the loss oscillate , converged and stop learning.


\section{Linear Regression(MLE approach) }
\section{Logistic Regression}
\[P(y=1|x) = h_\theta(x) = \frac{1}{1 + \exp(-\theta^\top x)} \equiv \sigma(\theta^\top x)\]
\[P(y=0|x) = 1 - P(y=1|x) = 1 - h_\theta(x)\]
Loss function is 
\[J(\theta) = -\sum_i \left[ y^{(i)} \log(h_\theta(x^{(i)})) + (1 - y^{(i)}) \log(1 - h_\theta(x^{(i)})) \right]\]
Again , BGD for following gradient of J : 
\[\frac{\partial J(\theta)}{\partial \theta_j} = \sum_i x^{(i)}j \left( h\theta(x^{(i)}) - y^{(i)} \right)\]
Interpretation : For a particular sample : if h return 1/0 and y return 1/0 , the term is 0. if  h return 1/0 and y return 0/1 , the term is positive infinity. 
\section{Logistic Regression(MLE approach)}
\section{Softmax Regression(Multi-Class Logistic)}
Softmax is used because it is differentiable 
k classes, n x k parameters , and the hypothesis is :

\begin{align}
h_\theta(x) =
\begin{bmatrix}
P(y = 1 | x; \theta) \\
P(y = 2 | x; \theta) \\
\vdots \\
P(y = K | x; \theta)
\end{bmatrix}
=
\frac{1}{ \sum_{j=1}^{K}{\exp(\theta^{(j)\top} x) }}
\begin{bmatrix}
\exp(\theta^{(1)\top} x ) \\
\exp(\theta^{(2)\top} x ) \\
\vdots \\
\exp(\theta^{(K)\top} x ) \\
\end{bmatrix}
\end{align}

Below Loss function  is quite easy to understand : By referencing to previous hpothesis, we want to maximize the y=k associated probility if that data belongs to class k
\[J(\theta) = -\left[ \sum_{i=1}^m \sum_{k=1}^K 1\{y^{(i)}=k\} \log \left( \frac{\exp(\theta^{(k)\top} x^{(i)})}{\sum_{j=1}^K \exp(\theta^{(j)\top} x^{(i)})} \right) \right]\]
Gradient of J is, we solve the problem by  GD : 
\[\nabla_{\theta^{(k)}} J(\theta) = -\sum_{i=1}^m \left[ x^{(i)} \left( 1\{y^{(i)}=k\} - P(y^{(i)}=k|x^{(i)};\theta) \right) \right]\]
Where : 
\[P(y^{(i)}=k|x^{(i)};\theta) = \frac{\exp(\theta^{(k)\top} x^{(i)})}{\sum_{j=1}^K \exp(\theta^{(j)\top} x^{(i)})}\]
\section{Regularization in Linear Regression}

% ######################################################################
\section{BGD variation : Mini BGD/SGD}
BGD use all training data in a single step, which is exteremely costly.
% ######################################################################
\section{Loss function in Classification(Binary) Problem - General treatment}
General Hypothesis : \(h_\theta(x) = x^T \theta\)
\newline
\newline
Adjustment for binary classification : 
\newline
\(
\text{sign}(h_\theta(x)) = \text{sign}(\theta^T x)= \text{sign}(t) = 
\begin{cases} 
1 & \text{if } t > 0 \\
0 & \text{if } t = 0 \\
-1 & \text{if } t < 0 
\end{cases}
\)
\newline
\newline
Measure of confidence :  \(h_\theta(x) = x^T \theta\) gives larger value, more confident  
\newline
\newline
Margin ( \(y x^T \theta\) ) : (i) if \(h_\theta(x)\) classify correctly, margin is positive, otherwise negative.
\newline
(ii) Therefore our objective is to maximize the margin ( we want both correct classification and be confident)
\newline
\newline
Consider the following loss function : 
\[
J(\theta) = \frac{1}{m} \sum_{i=1}^{m} \phi\left(y^{(i)} \theta^T x^{(i)}\right)
\]

We want penalize wrong classfication and encourage correct one , we design \(\phi\) as 
\(
\phi(z) \to 0 \text{ as } z \to \infty, \quad \text{while} \quad \phi(z) \to \infty \text{ as } z \to -\infty
\)  where  \(z = yx^{T}\theta\) , and examples are :  
\newline 
\newline
logistic loss : \(\phi_{\text{logistic}}(z) = \log(1 + e^{-z})\) ,used in logistic regression 
\newline 
\newline 
hinge loss : \(\phi_{\text{hinge}}(z) = [1 - z]_+ = \max{1 - z, 0}\), used in SVM
\newline 
\newline 
Exponential loss \(\phi_{\text{exp}}(z) = e^{-z}\), used in boosting 
\section{Kernel Mapping (Special case demo by Linear Regression + Polymoninal Kernel)}
(I) Purpose : To map x from lower higher dimension. Useful when data are non-linearly separable(Transform to a curve) 
\newline
(II) Computation complexity does not necessarily increase proportionately.
\newline
(III) Example : a mapping function \(\varphi : R \to R^{4}\) , \(x \to [1,x,x^{2},x^{3}]\), and h is \(\theta^{T}x\) having  \(\theta  = [\theta_1,\theta_2,\theta_3,\theta_4]\) 
\newline
(IV) Terms : x is called attribute, \(x \to [1,x,x^{2},x^{3}]\) called feature, \(\varphi\) feature map, \(\varphi : R^{1} \to R^{4}\) in this case. d =1 p = 4 
\newline
(IV) Another Example : a mapping function \(\varphi : R^{3} \to R^{1000}\) , \(x \to [1,x_1,x_1^{2},x_1^{3},x_1 x_2,x_1 x_2^{2} .... ]\) (*) ,let d  = 3 , p =1000. If we exhaust all possibilities, then p = 1 + d + \(d^2\) + \(d^3\)  (**) 
\newline
Recall GD stepping  : 
\[\theta := \theta + \alpha \sum_{i=1}^{n} (y^{(i)} - h_{\theta}(x^{(i)}))x^{(i)}\]
\[\theta := \theta + \alpha \sum_{i=1}^{n} (y^{(i)} - \theta^Tx^{(i)})x^{(i)}\]
Putting kernel mapping to the equation :
\[\theta := \theta + \alpha \sum_{i=1}^{n} (y^{(i)} - \theta^T\phi(x^{(i)}))\phi(x^{(i)})\]

We pause here to evaluate the cost of computing each of update (Curse of Demensionality...), considering (**). If we just use the kernel direction, we suffer the curse of demensionality : Suppose d (data dimension) = 1000, then by using the mapping in (**) we have p = \(10^9\). \(\theta^T\phi(x^{(i)})\) need O(p) (dot product ) , and O(np) for summing up all data in each step.
\newline
Going back to BGD.
\[\theta := \theta + \alpha \sum_{i=1}^{n} (y^{(i)} - \theta^T\phi(x^{(i)}))\phi(x^{(i)})\]
, assuming 
\(\theta = \sum_{i=1}^{n} \beta_i \phi(x^{(i)})\) (*) at some point, with initialization \(\theta = 0 = \beta \)
\newline 
It becomes 
\[\theta := \sum_{i=1}^{n} \beta_i \phi(x^{(i)}) + \alpha \sum_{i=1}^{n} (y^{(i)} - \theta^T\phi(x^{(i)}))\phi(x^{(i)})\] 
Rearranging : 
\[\theta := \sum_{i=1}^{n} (\beta_i + \alpha  (y^{(i)} - \theta^T\phi(x^{(i)})))\phi(x^{(i)})\] 
Therefore it is equivalent to updating \(\beta_i\) ( instead of \(\theta_i\) ) by 
\[\beta_i := \beta_i + \alpha  (y^{(i)} - \theta^T\phi(x^{(i)}))\]
by (*) above
\[ \beta_i := \beta_i + \alpha \left( y^{(i)} - \sum_{j=1}^{n} \beta_j \phi(x^{(j)})^T \phi(x^{(i)}) \right)\]
Computing of LHS is fast because : (1) we can pre-compute \( \phi(x^{(j)})^T \phi(x^{(i)})\) for all i,j, and (2) \( \phi(x^{(j)})^T \phi(x^{(i)})\) can be represented by \(<x^{(i)},x^{(j)}>\)  : 
\newline
\(\langle \phi(x), \phi(z) \rangle = 1 + \sum_{i=1}^{d} x_i z_i + \sum_{i,j \in \{1, \ldots, d\}} x_i x_j z_i z_j + \sum_{i,j,k \in \{1, \ldots, d\}} x_i x_j x_k z_i z_j z_k = 1 + \langle x, z \rangle + \langle x, z \rangle^2 + \langle x, z \rangle^3\) (**)
\newline 
\newline 
Define K where K is n x n ( n is the number of training samples) matrix, with \(K(x, z) = \langle \phi(x), \phi(z) \rangle\) , where \(K_ij\) is \(\langle \phi(x^{(i)}), \phi(x^{(j)}) \rangle\)
\newline\newline
Therefore, the process is : (1) compute \(K_ij\) using (**) , \(\text{for all } i, j \in \{1, \ldots, n\}. \quad \text{Set } \beta := 0\) ,
\newline
 (2) Loop
\[\forall i \in \{1, \ldots, n\}, \quad \beta_i := \beta_i + \alpha \left( y^{(i)} - \sum_{j=1}^{n} \beta_j K(x^{(i)}, x^{(j)}) \right)\] 
in vectorized notation: 
\[\beta := \beta + \alpha (\tilde{y} - K\beta)\]
When doing inference : 
\[\theta^T \phi(x) = \sum_{i=1}^{n} \beta_i \phi(x^{(i)})^T \phi(x) = \sum_{i=1}^{n} \beta_i K(x^{(i)}, x)\]
In practice, we do computation using K ( at O(d) cost ) instead of directly from \(\phi(x)\) is much faster. Further, We only need to know K but "just only need to know" the existence of \(\phi(x)\). There is no need to be able to write down \(\phi(x)\). Consider the Kernel applied to bitmap : number of bits as d. (Great reduction!)
\newline  
Intuitively, K represents similarity matrix, i.e. K is small if \( \phi(x^{(j)})^T \phi(x^{(i)})\) is small
\newline
Example : Gaussian Kernel, it can support inifinitely dimensional space of mapping.
\[K(x, z) = \exp\left(-\frac{||x - z||^2}{2\sigma^2}\right)\]
\newline 
Mercer Theorem : For K to be a valid Kernel iff K is PSD.
\newline 
Application : To SVM, perceptron, linear regression, and other learning algorithms represented only in inner product \(\langle x,z\rangle\) , then Apply K(x,z)  

 
\section{Generative vs Discriminative Learning Algorithm for classification, discussion} 
D : Learn the curve that separates the classes, e.g. Logistic Regression, SVM , ANN, CNN \newline
G : Learn (all the parameters of ) the model itself and just class of data. e.g. Naive Bayes, Gradient Discriminant Analysis, GAN\newline
An Analogy of G : Learn both English and French , and guess whether the word Bonjour is French or English. \newline
Another Example of G (*): \newline
Let's say a model is trained with 1000 pictures :\newline 
(i) Dog without glasses : 1 \newline
(ii) Dog with glasses : 239 \newline
(iii) Human without glasses : 500 \newline
(iv) Human with glasses : 260 \newline
Assume we have a photo with a glasses. To classify a dog or human in the picture for a generative model : Since P ( H \& G ) / P (G)= 260/261 \(>\) P(D \& G ) / P(G)= 1/261, the model infer that it is a human.\newline
let x be feature, y be class\newline
Put it in another way, in D  (e.g. (multi-class) logistic regression) , we learn h which is p(y\(|\)x) and infer the class with largest p(y\(|\)x). i.e. we are finding \(argmax_{y} p(y|x)\)\newline
In G, we are learning p(x\(|\)y) (by learning all p(x) for each possible classes of y  ) and p(y) (pdf of all classes of y ). Let y be the class (Dog=1 vs Human=0) , x be feature( with glasses ) , we learn p(x\(|\)y=1) ( case (i)/(ii) )  and p(x\(|\)y=0) ( case (iii)/(iv) )\newline 
Mathematically, D and G's relationship :  \(argmax_{y} p(y|x) [D] = argmax_{y} \frac{p(x|y)p(y)}{p(x)} = argmax_{y} p(x|y)p(y) [G] = argmax_{y} p(x \& y) \)\newline
(bayes rule, x is independent variable, bayes rule again, also see (*) )\newline
\section{Naive Bayes Classifier}
An example of Generative Learning algorithm Example usage , spam mail detection\newline
Let \(x_i\)'s be the all words in dictionary. y = 1 for spam mail, y = 0 for non-spam mail. \newline
In training, we want to learn the parmaters : \(\phi_y\) ( p of spam mail) , \(\phi_{j(y=1)}\) ( p of \(j^{th}\) word appearing in spam mail), and \(\phi_{j(y=0)}\) \newline
We have the following joint likelihood function 
\[L(\phi_y, \phi_{j(y=0)}, \phi_{j(y=1)}) = \prod_{i=1}^{n} p(x^{(i)} , y^{(i)})\]
\[
\phi_{j(y=1)} = \sum^{n}_{i=1} 1\{x_j^{(i)} = 1 \wedge y{(i)} = 1 \}
\]
\[
\phi_{j(y=0)} = \sum^{n}_{i=1} 1\{x_j^{(i)} = 1 \wedge y{(i)} = 0 \}
\]
\[
\phi_y = \sum^{n}_{i=1} 1\{ y{(i)} = 1 \}
\]
Above is just simple counting\newline\newline
For Inference, how ?\newline
A (non-)spam email having x's words has the probability :
\[
	p(x's|y) = p(x_1....x_{5000} |  y) = p(x_1|  y)p(x_2|y,x_1)p(x_3|y,x_2,x_1)....p(x_{5000}|y,x_2,x_1,....,x_{4999})
\]
\[
= p(x_1|  y)p(x_2|y)p(x_3|y)....p(x_{5000}|y) = \prod_{i=1}^{n} p(x_i \mid y_i)
\]
First by bayes rule(can be proved by induction), then by naives bayes assumption. e.g. \(p(x_{2087}|y) = p(x_(2087) | y,x_{39831})\) 

Finally compare \(p(y=1|x's)\) and \(p(y=0|x's)\) to determine whether it is a spam mail or not: \newline
By bayes rule, 
\[
	p(y = 1 | x's) = p(x's|y =1)p(y =1) /p(x's)  
\]
\[
= \frac{\prod_{j=1}^{d} p(x_j \mid y=1) p(y=1)}{\prod_{j=1}^{d} p(x_j \mid y=1) p(y=1) + \prod_{j=1}^{d} p(x_j \mid y=0) p(y=0)}
\]
\[
= \frac{\prod_{j=1}^{d} \phi_{j(y=1)} \phi_y}{\prod_{j=1}^{d} \phi_{j(y=1)} \phi_y + \prod_{j=1}^{d} \phi_{j(y=0)} (1-\phi_y)}
\]
Practically : (1) Remove common words in preprocessing. e.g. the , of (stop words) (2) Instead of labeling all words in dictionary, we build only from trained data. \newline\newline
\textbf{Laplace Smoothing} -- Handling unseen word\newline 
Problem :  both classes give zero in p(x|y) \newline
To solve :  Treat that new word to have appaeared in all classes once, good thing is that it won't change the relative p :\newline
\[
P(j \mid y=1) = \frac{1 + \sum_{i=1}^{n} 1\{x_j^{(i)}  = 1 \wedge y^{(i)} = 1\} }{2 + \sum_{i=1}^{n} 1\{ y^{(i)} = 1\} }
\]
\[
P(j \mid y=0) = \frac{1 + \sum_{i=1}^{n} 1\{x_j^{(i)}  = 1 \wedge y^{(i)} = 0\} }{2 + \sum_{i=1}^{n} 1\{ y^{(i)} = 0\} }
\]
\section{Bernoulli event model} 
\section{Gradient Discriminant Analysis} 
\section{Entropy} 
Definition : 
\[H(x) = -\sum_{i=1}^{n} p(x_i) \log_2 p(x_i)\] 
where \(x_1, x_2...\) are all possible events of random variable(distribution) and   \(p(x_1), p(x_2)...\) are the probabilities of the respective events.
\newline
\newline
Connection with uncertainty ( High Entropy , High Uncertainty ) :
\newline
\newline
Entropy measure the uncertainity of a distribution. Consider a random variable distribution :  X=1 at 0.33 , X = 2 at 0.33, X=3 at 0.33, and another : X = 1  at 0.98 , X = 2 at 0.01, X =3 at 0.01, we say the former distribution has higher uncertainity ( more difficult to guess its value ). 
\newline
\newline
Connection with amount of information in a *message* ( not distribution) :
\newline
\newline
Average number of bits (yes/no answers) NEEDED TO PROVIDE to tell x in a message. Therefore High Entropy. Higher Uncertainty , Higher Amount of Information. 
\newline
\newline
Connection with Decision Tree splitting : 
\newline
\newline
Remember that DT is greedy algorithm : It is to find the split that have greatest reduction in uncertainty ( Information Gain ) of the distribution ( after splitting ). We have a certain distribution 
\section{SVM, Support Vector Machine}
Assumptions for illustration : data in binary classes only, linearly separable (if not, then apply Kernel )
\newline
Main Idea in Training : We construct a separating hyperplane, the plane has largest distance to all data point. 
\newline 
How ? 
\newline
Let \(y \in \{-1, 1\}\) 
\newline
Define classifier \(h_{w,b}(x) = g(w^T x +b)\), where \(w^T x +b\) is the formula of hyperplane, w is the normal vector to hyperplane.
\newline 
where g : \(g(z) = 1 \text{ if } z \ge 0 ,  g(z) = -1 \text{ if } z < 0\)
\newline 
where w = \([\theta_1...\theta_n]^T\)
\newline 
Define functional margin (FM): \(\hat{\gamma}^{(i)} = y^{(i)} (w^T x + b)\) .
\newline 
If FM is positive, it classify correctly. Negative, classify incorrectly. 
\newline
If the magnitude is large , classifier gives highly confident result. 
\newline
Define Geometric Margin (GM), \(\gamma_i = \frac{\hat{\gamma^i}}{\|w\|}\)
\newline
Further define smallest distance from hyperplane to data points : \(\gamma = \min_{i=1, \ldots, m} \gamma^{(i)}\)
\newline 
Thus, our training objective is to maximize this smallest distance.
\[
\max_{\gamma, \mathbf{w}, b}  \quad \gamma \\
\]
\[
\text{s.t.}  \quad y^{(i)} (\mathbf{w}^T \mathbf{x}^{(i)} + b) \geq \gamma, \quad i = 1, \ldots, m \\
\]
\[
 \quad \|\mathbf{w}\| = 1
\]
The last condition ensure FM =\(>\)GM 
\newline
Why use GM instead of FM in training ? We can always scale w and b to achieve greater magnitude in FM, therefore FM is meaningless for training. 
\newline 
Rearranging :
\newline
\[
\max_{\gamma, \mathbf{w}, b}  \quad \hat{\gamma} / \quad \|\mathbf{w}\|
\]
\[
\text{s.t.}  \quad y^{(i)} (\mathbf{w}^T \mathbf{x}^{(i)} + b) \geq \hat{\gamma}, \quad i = 1, \ldots, m \
\]
In order to make it a convex optimization problem ( another subject to study ! F! )\newline
(1) we restrict the value of \(\hat{\gamma} = 1\) , by scaling w and b (can do single w/b for all \(\hat{\gamma} \) ?)  \newline
(2) and rewriting \(\frac{\hat{\gamma}}{\|\mathbf{w}\|} = \frac{1}{\|\mathbf{w}\|}\),
we  get w in from nominator to denominator
\[
\min_{\gamma, \mathbf{w}, b}  \quad \frac{1}{2} \|\mathbf{w}\|^2 \\
\]
\[
\text{s.t.} \quad y^{(i)} (\mathbf{w}^T \mathbf{x}^{(i)} + b) \geq 1, \quad i = 1, \ldots, m
\]
Apply cvx opt. library to solve the above problem 

\section{Bagging and Random Forest}
Review of Decision Tree : Greedy algorithm , Split on latest Information gain, Entropy/Gini Coefficient \newline
Bagging =  Bootstrapping + Aggregration \newline
Boostrapping = Resample with replacement, to generate different sample set of "same population"  \newline
Aggregration = perform averaging / voting with different Trees from different boostraping samples \newline
Aim to reduce variance \newline
Ramdom Forest : Bagging + randomly remove features in build indivudal trees \newline

\section{Adaboost}

Main Idea : Boosting transform weak learner to strong classifier, by increasing the weight of wrongly classified samples to force the classifier to do well on those samples.  It is kind of ensembling ( by attaching different weights to different classifiers ). 



Let \(\phi_{\tau} (x^{(i)} \) be a  weak learner (e.g. Decision Stump).
\[
h_\theta(x) = \text{sign} \left( \sum_{j=1}^n \theta_j \phi_j(x) \right)
\] 
is the hypothesis of boosting.\newline
Loss function (also check the general loss function discussion above) : \newline
\[
J(\theta) = \frac{1}{m} \sum_{i=1}^{m} \exp(-y^{(i)} \theta^T \phi(x^{(i)}))
\]
By Coordinate descent (choose a coordinate in \(\theta\)  and compute  
\[
\theta_j = \arg \min_{\theta_j} J(\theta)
\]


Specifically, the boosting algorithm performs coordinate descent on the exponential loss for classification problems. The objective:

Coordinate descent algorithm: \newline
1. Choose a coordinate \( j \in \{1, \ldots, N\} \)
2. Update \( \theta_j \): \( \theta_j = \arg \min_{\theta_j} J(\theta) \)
   Leave \( \theta_k \) unchanged for all \( k \neq j \)
   Iterate until convergence

Derivation of the coordinate update for coordinate \( j \):

$$J(\theta) = \frac{1}{m} \sum_{i=1}^{m} \exp(-y^{(i)} \theta^T \phi(x^{(i)}))$$

The objective function:

$$J(\theta) = \frac{1}{m} \sum_{i=1}^{m} \exp \left( -y^{(i)} \sum_{j=1}^{N} \theta_j \phi_j(x^{(i)}) \right)$$

Property of exp:

$$J(\theta) = \frac{1}{m} \sum_{i=1}^{m} w_i \exp \left( -y^{(i)} \theta_j \phi_j(x^{(i)}) \right)$$

Define \( w_i = \exp \left( -y^{(i)} \sum_{k \neq j} \theta_k \phi_k(x^{(i)}) \right) \)

To optimize coordinate \( \theta_j \) (***):

$$\theta_j = \arg \min_{\theta_j} \sum_{i=1}^{m} w_i \exp \left( -y^{(i)} \theta_j \phi_j(x^{(i)}) \right)$$

Define the weights (*) :

$$w_i = \exp \left( -y^{(i)} \sum_{k \neq j} \theta_k \phi_k(x^{(i)}) \right)$$ 
Important Note : by definition of (*),  updating \( \theta_j \) corresponds to updating w, therefore we say Adaboost is updating weights of samples \newline
By definition of (***) , it has the meaning of assigning weight \(w_i\) to sample \(x_i\), and we are finding \(\theta_j\) to do the best for classification (i.e. minimize the loss)\newline
Optimizing coordinate \( \theta_j \) corresponds to minimizing:

$$\sum_{i=1}^{m} w_i \exp \left( -y^{(i)} \theta_j \phi_j(x^{(i)}) \right)$$

Define:

$$w_i^+ := \sum_{i: y^{(i)} \phi_j(x^{(i)}) = 1} w_i$$
$$w_i^- := \sum_{i: y^{(i)} \phi_j(x^{(i)}) = -1} w_i$$


**************************\newline
Following is the whole algorithm  : \newline
**************************\newline
For each iteration \( t = 1, 2, \ldots \): \newline

(i) Define weights (**) \[ w^{(i)} = \exp \left( -y^{(i)} \sum_{\tau=1}^{t-1} \theta_{\tau} \phi_{\tau} (x^{(i)}) \right) \] 
and distribution, which is the weight (which is uniform initially) attached to each sample, we are tuning this : 

\[ p^{(i)} = \frac{w^{(i)}}{\sum_{j=1}^{m} w^{(j)}} \]  

(ii) Construct a weak hypothesis \( \phi_t : R^n \rightarrow \{-1,1\} \) from the distribution 
\[ p = \left( p^{(1)}, \ldots, p^{(m)} \right) \] 
on the training set. \newline

(iii) Compute \[ W^+_t = \sum_{i : y^{(i)} \phi_t (x^{(i)}) = 1} w^{(i)} \] 
and 
\[ W^-_t = \sum_{i : y^{(i)} \phi_t (x^{(i)}) = -1} w^{(i)} \] and set \[ \theta_t = \frac{1}{2} \log \frac{W^+_t}{W^-_t} . \] 
Final note here : when 
Proof of Boosting convergence and Discussion of weak learners : Omitted
\section{Gradient Boosting - Regression}
1. Fit F(x) to model data \newline 
2. Fit another \(h_1(x)\) to fit the residue in 1. : \((x_1,y_1-F(x_1),(x_2,y_2-F(x_2),(x_3,y_3-F(x_3)....)\). We got F+\(h_1\) \newline
3. Goes on to get \(F+h_1+h_2+h_3...\) \newline 
\newline
4. If we consider the loss function as \(L(y,F(x)) = (y - F(x))^2/2\), then we get \newline
\(\frac{\partial J}{\partial F(x_i)} = \frac{\partial}{\partial F(x_i)} \sum_{i} L(y_i, F(x_i)) = \frac{\partial L(y_i, F(x_i))}{\partial F(x_i)} =  y_i - F(x_i)  \) \newline 
We are actually doing Gradient descent! \newline 
5. Note here we are treating the model \(F(x_i)\) as random variable, i.e. we are updating F instead of \(x_i\). And data dimension = number of samples \newline
6. MSE may be susceptible by outliners. Adopt absolute loss L(y,F)=\(|y - F|\) or Huber loss: \newline
\[
L_{\delta}=
    \left\{\begin{matrix}
        \frac{1}{2}(y - \hat{y})^{2} & if \left | (y - \hat{y})  \right | < \delta\\
        \delta ((y - \hat{y}) - \frac1 2 \delta) & otherwise
    \end{matrix}\right.
\]
7. Choose proper learning reate (Omitted)

\section{Gradient Boosting - Classification}
As an Illustration , English character recognition : A-Z  : \newline
1. Generate feature vectors  x with k dimension, n samples , e.g. \(1^{st}\) = length of longest straight line,  \(2^{nd}\) = number of pixels .... \newline
2. Define 26 models \(F_A,F_B...F_Z\) , and corresponding \(P_A(x) .... P_Z(x) \)  by softmax , where
\[ P_A(x) = \frac{e^{F_A(x)}}{\sum_{c=A}^{Z} e^{F_c(x)}}\]
3. Define 26 "true" distributions (*) \(Y_c(x_i)\) , where \(x_i\) is \(i^{th}\) sammple. If \(y_5\) is G , then  \(Y_G(x_5)\) = 1 and \(Y_{c=/=G}(x_5)\) = 0. Notes : there are n x 26 values for it \newline
4. Here, we have n x 26 variables to optimize : \(F_A(x_1),F_A(x_2)....,F_A(x_{n-1}), F_Z(x_n)\). 
5. Use KL divergence as  loss function, by comparing currently adjusted F and (*).
\section{Gradient Boosting - Ranking}
\section{Bias Variance Analysis} 

\section{PCA, Principal Component Analysis}
We thnk that N dimensoinal data have certain "directions" - called principal axis (PA). We project those data points with reference to these axis/planes. \newline 
Generally, (i) we could intepret the meaning of the axis by comparing (data in differente  extreme of  the axis ), or  (ii) by mapping data to the axis - dimensionality reduction.\newline
(Stanford cs168 notes' explanation is awesome ...). \newline 
0. \(x_i\)'s are mapped to \(v_k\)'s ( eigenvector ), Hence, N th dmension is mapped to kth dimension , where generally N is greater than k \newline
1. The data keep the most variance (Retain the variability of data : Minimize information loss, minimize reconstruction error, remember the triangle ) if it is projected to PA(remember animation online). Or say, MAXIMIZE the projection length \newline 
\[ \sum_{i=1}^{n} \sum_{j=1}^{k} \langle x_i, v_j \rangle^2 \]
where v is the PA and x is the data point\newline
Equivalently:
\[ \underset{v : \|v\| = 1}{\arg\max} \frac{1}{n} \sum_{i=1}^{n} \langle x_i, v \rangle^2 \]
Since : 
\[ v^\top X^\top X v = (Xv)^\top (Xv) = \sum_{i=1}^{n} \langle x_i, v \rangle^2 \]
Also, by  Linear Algebra 
\[(Xv)^\top (Xv)  = vX^\top Xv\] , \(X^\top X\) is called corvariance matrix and is Positive Semi Definite and  can be written in \(QDQ^\top\) , where D is diagonal Matrix and Q is orthogonal Matrix ( \(Q^\top Q = I\) ), \newline
Let \(\lambda_1\)  be the lartest value in D \newline
To maximize \(v^\top_1 A v_1\), since 
\begin{align*}
v^\top_1 A v_1 &= v^\top_1 Q D Q^\top v_1 \\
&= e^\top_1 Q^\top Q D Q^\top Q e_1 \\
&= e^\top_1 D e_1 \\
&= \lambda_1
\end{align*}

For above shows that, the largest value of  \(v^\top_1 A v_1\) is \(\lambda_1\)(the largest eigenvalue) . And when \(v_1 = Qe_1\) , \(v^\top_1 A v_1\) attain largest value.  \newline 

To compute the projection (Power Iteration / SVD )

 Power Iteration
 Given matrix A =  \(X^\top X\):
 • Select random unit vector u0 \newline
 • For \( i = 1, 2, \dots \), set \( u_i = A^i u_0 \). If 
\[
\frac{u_i}{\|u_i\|} \approx \frac{u_{i-1}}{\|u_{i-1}\|},
\]
then return 
\[
\frac{u_i}{\|u_i\|}.
\]
i.e. keep multiplying  A to u until converge. (Analysis omitted, SVD method omitted)

\section{SVD, Singular Value Decomposition}

Definition : Any Matrix A ( m x n ) can be factorized as \(USV^T\), where \newline
U is m x m Orthogonal Matrix , Column of U called left singular vectors \newline
S is m x n Diagonal Matrix, entries of S called Orthogonal Matrix \newline
V is n x n  Orthogonal Matrix. Column of  V (Row of \(V^T\)) called right singular vectors of A \newline
\newline
\newline
Rank of Matrix : Minimum number of linear independent row or column in a Matrix \newline 
Rank Factorization: Let a matrix has Rank k , then A can be factorized as \(UV^T\) \newline 
where U is m x k and \(V^T\) is k x n \newline 
Low Rank Approximation : remove small K , therefore A is approximated to \(A_k =U_kS_kV^T\) \newline
Used in compression, fine tune LLM, De-noising, Matrix completion. \newline
Relationship between SVD and PCA (TODO CS168 notes) \newline


\section{Backpropagation}
\section{EM Algorithm}
\section{Reinforcement learning - Discrete State}
Markov decision processes (MDP) \newline
Definitions :  \newline
S : set of states. \newline 
A : set of  actions \newline 
\(P_{sa}\) : denote the set of  probability of action a on state s to change to another state. \newline
\(\gamma\) between [0-1) as discount factor, to punish (decrease value function when time pass) \newline


\section{Reinforcement learning - Continuous State}
\section{MAP (Maximum a Posterior) vs MLE (Maximum Likelihood Estimation)} 
\section{IDP, Indepent Component Analysis}

\section{Hidden Markov Model}
\section{Apriori}
\section{Recommender System}
\section{Anomaly Detection} 
\section{Perceptron}
\section{KL Divergence } 
\[KL(P||Q)=\sum_{x}P(x)\log(\frac{P(x)}{Q(x)})\] 
1. Measure the difference betwwen 2 distributions : You think it is Q, but actually it is P  \newline
2. The above is for discrete distribution \newline
3. KLD is assymetric \newline
4. In ML , measure the Information loss if Q is used instead of P. In other words, It measures the information loss (Entropy increased ) if Q is used to approximate the true distribution (P).  
\section{Cross Entropy}
\[H(p,q)=-\sum_x p(x)\log q(x).\]
1. It measures the average number of bits required to identify an event from one probability distribution, p, using the optimal code for another probability distribution, q. \newline 
2. The large the difference in bit numbers : we consider it as much difference. \newline
2. As loss function for classification , e.g. logistic regression , CNN classifier. \newline
\section{Cross Validation}


\end{document}
